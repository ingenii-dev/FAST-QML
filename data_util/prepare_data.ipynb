{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20107312-742e-4c3c-8816-452e0cbe12e2",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "In this notebook we are going to load the PDBBind dataset and prepare it for the training phase.\n",
    "\n",
    "## 1. Download dataset\n",
    "The PDBBind dataset can be found in [pdbbind 2016 datasets](http://www.pdbbind.org.cn). You will be asked to register (or log in) under a license agreement. Then, go to the CASFtab and download the CASF-2016 file. This file includes three folders: general-except-refined, refined and core. The general set contains 13308 protein−ligand binding complexes in total , the refined set contains 4057 complexes and the core set contains 290 complexes. The refined set contains less complex-sized data. Both the general and refined sets can be used for training and validation. The core set will be used for testing. Notice that you can also download the dataset by going to the Download tab and scrolling down to the PDBbind v2016 sign. Notice, however, that the core set is not included in this tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7677c52-71a6-499d-b261-744372774d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bef233-f169-49c0-8c7e-0e82533f23e8",
   "metadata": {},
   "source": [
    "## 2. Extract csv from index files in pdbbind\n",
    "\n",
    "We extract the names of the folders containing the different complexes and store them in a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "890c2f7b-673a-4b06-9f13-a6711890127c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the extracted PDBbind dataset\n",
    "path = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "475a97ed-b0ae-4d46-a1a8-1df691daa8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s $path --out missing\n",
    "\n",
    "path=$1\n",
    "\n",
    "echo 'pdbid,-logKd/Ki' > affinity_data.csv\n",
    "cat $path/general-set-except-refined/index/INDEX_general_PL_data.2016 | while read l1 l2 l3 l4 l5; do\n",
    "    if [[ ! $l1 =~ \"#\" ]]; then\n",
    "        echo $l1,$l4\n",
    "    fi\n",
    "done >> affinity_data.csv\n",
    "\n",
    "\n",
    "# Find affinities without structural data (i.e. with missing directories)\n",
    "\n",
    "cut -f 1 -d ',' affinity_data.csv | tail -n +2 | while read l;\n",
    "    do if [ ! -e $path/general-set-except-refined/$l ] && [ ! -e $path/refined-set/$l ]; then \n",
    "        echo $l;\n",
    "    fi\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a87ad4a1-cca2-44bc-9d8c-247ded5253fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing = set(missing.split())\n",
    "len(missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a153bd4b-a6b1-4773-8dd3-ec2166935541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pdbid</th>\n",
       "      <th>-logKd/Ki</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3zzf</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3gww</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1w8l</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3fqa</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1zsb</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  pdbid  -logKd/Ki\n",
       "0  3zzf       0.40\n",
       "1  3gww       0.45\n",
       "2  1w8l       0.49\n",
       "3  3fqa       0.49\n",
       "4  1zsb       0.60"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "affinity_data = pd.read_csv('affinity_data.csv', comment='#')\n",
    "affinity_data = affinity_data[~np.in1d(affinity_data['pdbid'], list(missing))]\n",
    "affinity_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9da3c8ce-5c79-4ddd-af41-d7ff5606b085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for NaNs\n",
    "affinity_data['-logKd/Ki'].isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadf480f-5b5b-41d7-bd8a-6e28e51979d9",
   "metadata": {},
   "source": [
    "We separate the refined, general and core sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d5b6e83-bd30-4d5d-b00f-0e647f93ea03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13307, 4057, 285)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "core_set = set([ f.path[11:] for f in os.scandir('./core-set') if f.is_dir() ])#set(core_set)\n",
    "\n",
    "refined_set = ! grep -v '#' $path/general-set-except-refined/index/INDEX_refined_data.2016 | cut -f 1 -d ' '\n",
    "refined_set = set(refined_set)\n",
    "\n",
    "general_set = set(affinity_data['pdbid'])\n",
    "\n",
    "\n",
    "assert core_set & refined_set == core_set\n",
    "assert refined_set & general_set == refined_set\n",
    "\n",
    "len(general_set), len(refined_set), len(core_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea38b934-31ce-474d-8d26-5c62aa0acfcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pdbid</th>\n",
       "      <th>-logKd/Ki</th>\n",
       "      <th>set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3zzf</td>\n",
       "      <td>0.40</td>\n",
       "      <td>general</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3gww</td>\n",
       "      <td>0.45</td>\n",
       "      <td>general</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1w8l</td>\n",
       "      <td>0.49</td>\n",
       "      <td>general</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3fqa</td>\n",
       "      <td>0.49</td>\n",
       "      <td>general</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1zsb</td>\n",
       "      <td>0.60</td>\n",
       "      <td>general</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  pdbid  -logKd/Ki      set\n",
       "0  3zzf       0.40  general\n",
       "1  3gww       0.45  general\n",
       "2  1w8l       0.49  general\n",
       "3  3fqa       0.49  general\n",
       "4  1zsb       0.60  general"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "affinity_data.loc[np.in1d(affinity_data['pdbid'], list(general_set)), 'set'] = 'general'\n",
    "\n",
    "affinity_data.loc[np.in1d(affinity_data['pdbid'], list(refined_set)), 'set'] = 'refined'\n",
    "\n",
    "affinity_data.loc[np.in1d(affinity_data['pdbid'], list(core_set)), 'set'] = 'core'\n",
    "\n",
    "affinity_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fd98c9-68d1-4544-8e87-c18ec48d5dea",
   "metadata": {},
   "source": [
    "## 3. Separate training, validation and test sets\n",
    "\n",
    "For the general and refined sets, we compute binding affinity quintiles of each set independently, then sample 10% of the data from each quintile to form each\n",
    "validation set for general and refined, with the remaining data kept for the respective general and refined training sets. As the result of our protocol, the general and refined sets are partitioned into general-train, general-val, refined-train, and refined-val."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf19f861-0521-4c41-9abf-19e1245cec7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "general = affinity_data[affinity_data.set=='general']\n",
    "refined = affinity_data[affinity_data.set=='refined']\n",
    "core = affinity_data[affinity_data.set=='core']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4a70f7c-731b-4ea5-b974-128432af6727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General set\n",
    "qs = np.quantile(general['-logKd/Ki'], np.arange(0,1.1,0.1)) #Get quantiles\n",
    "qs[0]-=0.01\n",
    "idxs_train = []\n",
    "idxs_val = []\n",
    "for i in range(len(qs)-1):\n",
    "    q1 = qs[i]\n",
    "    q2 = qs[i+1]\n",
    "    gen_q = general[(general['-logKd/Ki']>q1) & (general['-logKd/Ki']<=q2)].index.values.tolist()\n",
    "    size_train = int(0.9*len(gen_q)) # 90% training, 10% test\n",
    "    idx_train = random.sample(gen_q,size_train)\n",
    "    idx_val = [idx for idx in gen_q if idx not in idx_train]\n",
    "    idxs_train+=idx_train\n",
    "    idxs_val+=idx_val\n",
    "general_train = general.loc[idxs_train]\n",
    "general_train.set = 'general_train'\n",
    "general_val = general.loc[idxs_val]\n",
    "general_val.set = 'general_val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf2e335e-88cf-4990-8a82-88db4bb6a5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refined set\n",
    "qs = np.quantile(refined['-logKd/Ki'], np.arange(0,1.1,0.1)) #Get quantiles\n",
    "qs[0]-=0.01\n",
    "idxs_train = []\n",
    "idxs_val = []\n",
    "for i in range(len(qs)-1):\n",
    "    q1 = qs[i]\n",
    "    q2 = qs[i+1]\n",
    "    ref_q = refined[(refined['-logKd/Ki']>q1) & (refined['-logKd/Ki']<=q2)].index.values.tolist()\n",
    "    size_train = int(0.9*len(ref_q)) # 90% training, 10% test\n",
    "    idx_train = random.sample(ref_q,size_train)\n",
    "    idx_val = [idx for idx in ref_q if idx not in idx_train]\n",
    "    idxs_train+=idx_train\n",
    "    idxs_val+=idx_val\n",
    "refined_train = refined.loc[idxs_train]\n",
    "refined_train.set = 'refined_train'\n",
    "refined_val = refined.loc[idxs_val]\n",
    "refined_val.set = 'refined_val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d621e39-d300-49a3-9323-fa0995add661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pdbid</th>\n",
       "      <th>-logKd/Ki</th>\n",
       "      <th>set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>823</th>\n",
       "      <td>4a4e</td>\n",
       "      <td>3.32</td>\n",
       "      <td>general_train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>3nk8</td>\n",
       "      <td>3.00</td>\n",
       "      <td>general_train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>4h4b</td>\n",
       "      <td>2.70</td>\n",
       "      <td>general_train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>1gbq</td>\n",
       "      <td>2.46</td>\n",
       "      <td>general_train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>3fty</td>\n",
       "      <td>3.21</td>\n",
       "      <td>general_train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13226</th>\n",
       "      <td>5dwr</td>\n",
       "      <td>11.22</td>\n",
       "      <td>core</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13238</th>\n",
       "      <td>4f2w</td>\n",
       "      <td>11.30</td>\n",
       "      <td>core</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13241</th>\n",
       "      <td>2x00</td>\n",
       "      <td>11.33</td>\n",
       "      <td>core</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13269</th>\n",
       "      <td>3o9i</td>\n",
       "      <td>11.82</td>\n",
       "      <td>core</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13270</th>\n",
       "      <td>4f3c</td>\n",
       "      <td>11.82</td>\n",
       "      <td>core</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13307 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      pdbid  -logKd/Ki            set\n",
       "823    4a4e       3.32  general_train\n",
       "544    3nk8       3.00  general_train\n",
       "358    4h4b       2.70  general_train\n",
       "250    1gbq       2.46  general_train\n",
       "709    3fty       3.21  general_train\n",
       "...     ...        ...            ...\n",
       "13226  5dwr      11.22           core\n",
       "13238  4f2w      11.30           core\n",
       "13241  2x00      11.33           core\n",
       "13269  3o9i      11.82           core\n",
       "13270  4f3c      11.82           core\n",
       "\n",
       "[13307 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "affinity_data = pd.concat([general_train, general_val, refined_train, refined_val, core])\n",
    "affinity_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9824a427-15a0-4d75-ab69-c2f1767dd744",
   "metadata": {},
   "outputs": [],
   "source": [
    "affinity_data.to_csv('../data/affinity_data_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ee15d8-0b5c-440a-9b22-5e4eb3441987",
   "metadata": {},
   "source": [
    "## 4. Prepare complexes with chimera\n",
    "\n",
    "We begin with the preprocessing of the data. All protein−ligand binding complexes are protonated and charges solved using UCSF Chimera22 with AMBER ff14SB23\n",
    "for standard residues and AM1-BCC24 for nonstandard residues, the default settings for the program. No additional steps are taken for crystal structure data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ea3e46-814b-48c5-9ce4-f59ef5ca8b60",
   "metadata": {},
   "source": [
    "We store all the .pdb files in a csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c8c1f05-c74b-41ea-9888-6e193abfff55",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "\n",
    "find . -name *.pdb >> pbd_files.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41deec78-18a2-4320-a4fc-678a4c4c4283",
   "metadata": {},
   "source": [
    "Now we run the preprocessing step. This will create .mol2 a file for each .pdb file. Notice that this can take a while (36h for me)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61bbb6e9-9757-4b88-a0e4-258c02e2b7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the extracted PDBbind dataset\n",
    "path = './pbd_files.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dde0fc-00de-48d9-bba7-2ddaf64eea98",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s $path\n",
    "path=$1\n",
    "\n",
    "# get list of pdb files from stdin and iterate over them. each instance of this script appends\n",
    "# its PID to the tmp.mol2 file in order to prevent race conditions, enabling this to be run with\n",
    "# gnu parallel\n",
    "\n",
    "        tmp_file=$$_tmp.mol2\n",
    "\n",
    "        echo \"my tmp file is ${tmp_file}\"\n",
    "\n",
    "cat $path | while read pdbfile; do\n",
    "\n",
    "                echo ${pdbfile}\n",
    "                mol2file=${pdbfile%pdb}mol2\n",
    "                # NOTICED THAT SOME INPUTS seem to never finish chimera step\n",
    "                echo -e \"open ${pdbfile} \\n addh \\n addcharge \\n write format mol2 0 $$_tmp.mol2 \\n stop\" | chimera --silent --nogui \n",
    "                # Do not use TIP3P atom types, pybel cannot read them\n",
    "                sed 's/H\\.t3p/H    /' ${tmp_file} | sed 's/O\\.t3p/O\\.3  /' > $mol2file\n",
    "\n",
    "\n",
    "done\n",
    "echo \"finished processing\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2a3257-63ff-49fc-bead-e0c7dfe914f2",
   "metadata": {},
   "source": [
    "## 5. Generate hdf5 from the csv and the original pdbbind dataset\n",
    "\n",
    "We extract the features from each complex and store them all in a hdf file.\n",
    "\n",
    "We consider only the heavy atoms from each biological structure and heteroatoms (e.g., oxygens from crystallized water molecules).\n",
    "• Element type: one-hot encoding of B, C, N, O, P, S, Se, halogen, or metal.\n",
    "• Atom hybridization (1, 2, or 3).\n",
    "• Number of heavy atom bonds (i.e., heavy valence).\n",
    "• Structural properties: bit vector (1 where present) encoding of hydrophobic, aromatic, acceptor, donor, ring.\n",
    "• Partial charge.\n",
    "• Molecule type to indicate protein atom versus ligand atom (−1 for protein, 1 for ligand).\n",
    "• Van der Waals radius.\n",
    "\n",
    "We use OpenBabel to do this (version 3.X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2c26b3-4aac-43f3-908e-da8d694b2649",
   "metadata": {},
   "source": [
    "We begin by relocating all the .mol2 files to the folder data/raw/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd5cc93-4f1f-4806-9d25-6bc2420dea7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "find . -name *ligand.mol2 >> ligand_mol2_files.csv\n",
    "find . -name *pocket.mol2 >> pocket_mol2_files.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df831c83-4eba-44ae-a346-753ee05d25d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "path='ligand_mol2_files.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc374c2-4d57-4a8d-aeb4-74fae288a262",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s $path\n",
    "path=$1\n",
    "cat $path | while read pdbfile; do\n",
    "        #echo ${pdbfile}\n",
    "        cp -R ${pdbfile} '../data/raw_data/'\n",
    "done\n",
    "echo \"finished processing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e1c5c9-074b-4f01-9489-1e13adfec47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path='pocket_mol2_files.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab510a1a-e01a-4497-931f-d1c05649ee9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s $path\n",
    "path=$1\n",
    "cat $path | while read pdbfile; do\n",
    "        #echo ${pdbfile}\n",
    "        cp -R ${pdbfile} '../data/raw_data/'\n",
    "done\n",
    "echo \"finished processing\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07e9732-d5d0-4ccd-982f-c7fd4c6d40e9",
   "metadata": {},
   "source": [
    "Now we run the script to extract the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b22a108-114c-4a90-a932-7495b560cc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tf_bio_data import Featurizer\n",
    "import numpy as np\n",
    "import h5py\n",
    "import argparse\n",
    "from openbabel import pybel as pb\n",
    "import warnings\n",
    "#from data_generator.atomfeat_util import read_pdb, rdkit_atom_features, rdkit_atom_coords\n",
    "#from data_generator.chem_info import g_atom_vdw_ligand, g_atom_vdw_protein\n",
    "import xml.etree.ElementTree as ET\n",
    "from rdkit.Chem.rdmolfiles import MolFromMol2File\n",
    "import rdkit\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdchem\n",
    "from openbabel.pybel import Atom\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "\n",
    "ob_log_handler = pb.ob.OBMessageHandler()\n",
    "ob_log_handler.SetOutputLevel(0)\n",
    "\n",
    "# TODO: compute rdkit features and store them in the output hdf5 file\n",
    "# TODO: instead of making a file for each split, squash into one?\n",
    "\n",
    "\n",
    "# TODO: not sure setting these to defaults is a good idea...\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--input-pdbbind\", default=\"/workspace/data/raw_data/\") ## Change this path for the path of your *ligand.mol2 files and *pocket.mol2\n",
    "parser.add_argument(\"--input-docking\", default=\"/workspace/data/raw_data/\")\n",
    "parser.add_argument(\"--use-docking\", default=False, action=\"store_true\") # No docking\n",
    "parser.add_argument(\"--use-exp\", default=True, action=\"store_true\")\n",
    "parser.add_argument(\"--output\", default=\"/workspace/data/processed\") # Path to store the processed hdf\n",
    "parser.add_argument(\n",
    "    \"--metadata\", default=\"/workspace/data/affinity_data_cleaned.csv\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "def parse_element_description(desc_file):\n",
    "    element_info_dict = {}\n",
    "    element_info_xml = ET.parse(desc_file)\n",
    "    for element in element_info_xml.iter():\n",
    "        if \"comment\" in element.attrib.keys():\n",
    "            continue\n",
    "        else:\n",
    "            element_info_dict[int(element.attrib[\"number\"])] = element.attrib\n",
    "\n",
    "    return element_info_dict\n",
    "\n",
    "\n",
    "def parse_mol_vdw(mol, element_dict):\n",
    "    vdw_list = []\n",
    "\n",
    "    if isinstance(mol, pb.Molecule):\n",
    "        for atom in mol.atoms:\n",
    "            # NOTE: to be consistent between featurization methods, throw out the hydrogens\n",
    "            if int(atom.atomicnum) == 1:\n",
    "                continue\n",
    "            if int(atom.atomicnum) == 0:\n",
    "                continue\n",
    "            else:\n",
    "                vdw_list.append(\n",
    "                    float(element_dict[atom.atomicnum][\"vdWRadius\"]))\n",
    "\n",
    "    elif isinstance(mol, rdkit.Chem.rdchem.Mol):\n",
    "        for atom in mol.GetAtoms():\n",
    "            # NOTE: to be consistent between featurization methods, throw out the hydrogens\n",
    "            if int(atom.GetAtomicNum()) == 1:\n",
    "                continue\n",
    "            else:\n",
    "                vdw_list.append(\n",
    "                    float(element_dict[atom.GetAtomicNum()][\"vdWRadius\"]))\n",
    "    else:\n",
    "        raise RuntimeError(\"must provide a pybel mol or an RDKIT mol\")\n",
    "\n",
    "    return np.asarray(vdw_list)\n",
    "\n",
    "\n",
    "def featurize_pybel_complex(ligand_mol, pocket_mol, name, dataset_name):\n",
    "\n",
    "    featurizer = Featurizer()\n",
    "    charge_idx = featurizer.FEATURE_NAMES.index('partialcharge')\n",
    "\n",
    "    # get ligand features\n",
    "    ligand_coords, ligand_features = featurizer.get_features(\n",
    "        ligand_mol, molcode=1)\n",
    "\n",
    "    # ensures that partial charge on all atoms is non-zero?\n",
    "    if not (ligand_features[:, charge_idx] != 0).any():\n",
    "        raise RuntimeError(\n",
    "            \"invalid charges for the ligand {} ({} set)\".format(name, dataset_name))\n",
    "\n",
    "    # get processed pocket features\n",
    "    pocket_coords, pocket_features = featurizer.get_features(\n",
    "        pocket_mol, molcode=-1)\n",
    "    if not (pocket_features[:, charge_idx] != 0).any():\n",
    "        raise RuntimeError(\n",
    "            \"invalid charges for the pocket {} ({} set)\".format(name, dataset_name))\n",
    "\n",
    "    # center the coordinates on the ligand coordinates\n",
    "    centroid_ligand = ligand_coords.mean(axis=0)\n",
    "    ligand_coords -= centroid_ligand\n",
    "\n",
    "    pocket_coords -= centroid_ligand\n",
    "    data = np.concatenate((np.concatenate((ligand_coords, pocket_coords)),\n",
    "                           np.concatenate((ligand_features, pocket_features))), axis=1)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    affinity_data = pd.read_csv(args.metadata)\n",
    "\n",
    "    element_dict = parse_element_description(\"./elements.xml\")\n",
    "\n",
    "    failure_dict = {\"name\": [], \"partition\": [], \"set\": [], \"error\": []}\n",
    "\n",
    "    for dataset_name, data in tqdm(affinity_data.groupby('set')):\n",
    "        print(\"found {} complexes in {} set\".format(len(data), dataset_name))\n",
    "\n",
    "        if not os.path.exists(args.output):\n",
    "            os.makedirs(args.output)\n",
    "\n",
    "        with h5py.File('%s/%s.hdf' % (args.output, dataset_name), 'w') as f:\n",
    "\n",
    "            for idx, row in tqdm(data.iterrows(), total=data.shape[0]):\n",
    "\n",
    "                name = row['pdbid']\n",
    "\n",
    "                affinity = row['-logKd/Ki']\n",
    "\n",
    "                #receptor_path = row['receptor_path']\n",
    "\n",
    "                '''\n",
    "                    here is where the ligand(s) for both the experimental structure and the docking data need to be loaded.\n",
    "                    * In order to do this, need an input path for both the experimental data as well as the docking data\n",
    "                    * For docking data:\n",
    "                        > Need to know how many poses there are, potentially up to 10 but not always the case\n",
    "                        > May not have ligand/pocket data for names, need to handle this possibility\n",
    "\n",
    "                    ######################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "                            BREAK THE MAIN LOOP INTO TWO PARTS....PROCESS DOCKING and PROCESS CRYSTAL STRUCTURES\n",
    "\n",
    "\n",
    "\n",
    "                    ######################################################################################################\n",
    "\n",
    "                '''\n",
    "\n",
    "                ############################## CREATE THE PDB GROUP ##################################################\n",
    "                # this is here in order to ensure any dataset that is created has passed the quality check, i.e. no failed complexes enter the output file\n",
    "\n",
    "                grp = f.create_group(str(name))\n",
    "                grp.attrs['affinity'] = affinity\n",
    "                pybel_grp = grp.create_group(\"pybel\")\n",
    "                processed_grp = pybel_grp.create_group(\"processed\")\n",
    "\n",
    "                ############################### PROCESS THE DOCKING DATA ###############################\n",
    "                if args.use_docking:\n",
    "                    # READ THE DOCKING LIGAND POSES\n",
    "\n",
    "                    # pose_path_list = glob(\"{}/{}/{}_ligand_pose_*.pdb\".format(args.input_docking, name, name))\n",
    "                    pose_path_list = glob(\n",
    "                        \"{}/{}/{}_ligand_pose_*.mol2\".format(args.input_docking, name, name))\n",
    "\n",
    "                    # if there are poses to read then we will read them, otherwise skip to the crystal structure loop\n",
    "                    if len(pose_path_list) > 0:\n",
    "\n",
    "                        # READ THE DOCKING POCKET DATA\n",
    "\n",
    "                        #docking_pocket_file = \"{}/{}/{}_pocket.mol2\".format(args.input_docking, name, name)\n",
    "                        docking_pocket_file = receptor_path\n",
    "\n",
    "                        if not os.path.exists(docking_pocket_file):\n",
    "                            warnings.warn(\"{} does not exists...this is likely due to failure in chimera preprocessing step, skipping to next complex...\".format(\n",
    "                                docking_pocket_file))\n",
    "                            # NOTE: not putting a continue here because there may be crystal structure data\n",
    "                        else:\n",
    "\n",
    "                            # some docking files are corrupt (have nans for coords) and pybel doesn't do a great job of handling that\n",
    "                            with open(docking_pocket_file, 'r') as handle:\n",
    "                                data = handle.read()\n",
    "                                if \"nan\" in data:\n",
    "                                    warnings.warn(\"{} contains corrupt data, nan's\".format(\n",
    "                                        docking_pocket_file))\n",
    "                                    # continue #TODO: THIS MAY PREVENT THE CRYSTAL STRUCTURE DATA FROM BEING PROCESSED\n",
    "\n",
    "                                else:\n",
    "\n",
    "                                    pose_pocket_vdw = []\n",
    "\n",
    "                                    try:\n",
    "                                        #docking_pocket = next(pybel.readfile('pdb', docking_pocket_file))\n",
    "                                        docking_pocket = next(pybel.readfile(\n",
    "                                            'mol2', docking_pocket_file))\n",
    "                                        pose_pocket_vdw = parse_mol_vdw(\n",
    "                                            mol=docking_pocket, element_dict=element_dict)\n",
    "\n",
    "                                    except StopIteration:\n",
    "                                        error = \"pybel failed to read {} docking pocket file\".format(\n",
    "                                            name)\n",
    "                                        warnings.warn(error)\n",
    "                                        failure_dict[\"name\"].append(name), failure_dict[\"partition\"].append(\n",
    "                                            \"docking\"), failure_dict[\"set\"].append(dataset_name), failure_dict[\"error\"].append(error)\n",
    "\n",
    "                                    # in some, albeit strange, cases the pocket consists purely of hydrogen, skip over these if that is the case\n",
    "                                    if len(pose_pocket_vdw) < 1:\n",
    "                                        error = \"{} docking pocket contains no heavy atoms, unable to store vdw radii\".format(\n",
    "                                            name)\n",
    "                                        warnings.warn(error)\n",
    "                                        failure_dict[\"name\"].append(name), failure_dict[\"partition\"].append(\n",
    "                                            \"docking\"), failure_dict[\"set\"].append(dataset_name), failure_dict[\"error\"].append(error)\n",
    "\n",
    "                                    else:\n",
    "\n",
    "                                        docking = processed_grp.create_group(\n",
    "                                            \"docking\")\n",
    "                                        for pose_path in pose_path_list:\n",
    "\n",
    "                                            try:\n",
    "                                                #pose_ligand = next(pybel.readfile('pdb', pose_path))\n",
    "                                                pose_ligand = next(\n",
    "                                                    pb.readfile('mol2', pose_path))\n",
    "                                                # do not add the hydrogens! they were already added in chimera and it would reset the charges\n",
    "                                            except:\n",
    "                                                error = \"no ligand for {} ({} set)\".format(\n",
    "                                                    name, dataset_name)\n",
    "                                                warnings.warn(error)\n",
    "                                                failure_dict[\"name\"].append(name), failure_dict[\"partition\"].append(\n",
    "                                                    \"docking\"), failure_dict[\"set\"].append(dataset_name), failure_dict[\"error\"].append(error)\n",
    "                                                continue  # TODO:THIS MAY PREVENT THE CRYSTAL STRUCTURE DATA FROM BEING PROCESSED\n",
    "\n",
    "                                            # extract the van der waals radii for the ligand/pocket\n",
    "                                            pose_ligand_vdw = parse_mol_vdw(\n",
    "                                                mol=pose_ligand, element_dict=element_dict)\n",
    "\n",
    "                                            # in case the ligand consists purely of hydrogen, skip over these if that is the case\n",
    "                                            if len(pose_ligand_vdw) < 1:\n",
    "                                                error = \"{} ligand consists purely of hydrogen, no heavy atoms to featurize\".format(\n",
    "                                                    name)\n",
    "                                                warnings.warn(error)\n",
    "                                                failure_dict[\"name\"].append(name), failure_dict[\"partition\"].append(\n",
    "                                                    \"docking\"), failure_dict[\"set\"].append(dataset_name), failure_dict[\"error\"].append(error)\n",
    "                                                continue  # TODO: THIS MAY PREVENT THE CRYSTAL STRUCTURE DATA FROM BEING PROCESSED\n",
    "\n",
    "                                            try:\n",
    "                                                pose_data = featurize_pybel_complex(\n",
    "                                                    ligand_mol=pose_ligand, pocket_mol=docking_pocket, name=name, dataset_name=dataset_name)\n",
    "                                            except RuntimeError as error:\n",
    "                                                failure_dict[\"name\"].append(name), failure_dict[\"partition\"].append(\n",
    "                                                    \"docking\"), failure_dict[\"set\"].append(dataset_name), failure_dict[\"error\"].append(error)\n",
    "                                                continue  # TODO:THIS MAY PREVENT THE CRYSTAL STRUCTURE DATA FROM BEING PROCESSED\n",
    "\n",
    "                                            pose_ligand_pocket_vdw = np.concatenate([pose_ligand_vdw.reshape(-1),\n",
    "                                                                                     pose_pocket_vdw.reshape(-1)], axis=0)\n",
    "\n",
    "                                            # enforce a constraint that the number of atoms for which we have features is equal to number for which we have VDW radii\n",
    "                                            assert pose_ligand_pocket_vdw.shape[0] == pose_data.shape[0]\n",
    "\n",
    "                                            # CREATE THE DOCKING POSE GROUP\n",
    "                                            #pose_idx = pose_path.split(\".pdb\")[0].split(\"_\")[-1]\n",
    "                                            pose_idx = pose_path.split(\n",
    "                                                \".mol2\")[0].split(\"_\")[-1]\n",
    "                                            pose_grp = docking.create_group(\n",
    "                                                pose_idx)\n",
    "\n",
    "                                            # Now that we have passed the try/except blocks, featurize and store the docking data\n",
    "                                            pose_grp.attrs[\"van_der_waals\"] = pose_ligand_pocket_vdw\n",
    "\n",
    "                                            pose_dataset = pose_grp.create_dataset(\"data\", data=pose_data,\n",
    "                                                                                   shape=pose_data.shape, dtype='float32', compression='lzf')\n",
    "\n",
    "                else:\n",
    "                    error = \"{} does not contain any pose data\".format(name)\n",
    "                    # tqdm.write(error)\n",
    "                    # failure_dict[\"name\"].append(name), failure_dict[\"partition\"].append(\n",
    "                    #    \"docking\"), failure_dict[\"set\"].append(dataset_name), failure_dict[\"error\"].append(error)\n",
    "\n",
    "                ############################### PROCESS THE CRYSTAL STRUCTURE DATA ###############################\n",
    "\n",
    "                if args.use_exp:\n",
    "                    # BEGIN QUALITY CONTROL: do not create the dataset until data has been verified\n",
    "                    try:\n",
    "                        crystal_ligand = next(pb.readfile(\n",
    "                            'mol2', '%s/%s_ligand.mol2' % (args.input_pdbbind, name)))\n",
    "\n",
    "                    # do not add the hydrogens! they were already added in chimera and it would reset the charges\n",
    "                    except:\n",
    "                        error = \"no ligand for {} ({} set)\".format(\n",
    "                            name, dataset_name)\n",
    "                        warnings.warn(error)\n",
    "                        failure_dict[\"name\"].append(name), failure_dict[\"partition\"].append(\n",
    "                            \"crystal\"), failure_dict[\"set\"].append(dataset_name), failure_dict[\"error\"].append(error)\n",
    "                        continue\n",
    "\n",
    "                    try:\n",
    "                        crystal_pocket = next(pb.readfile(\n",
    "                            'mol2', '%s/%s_pocket.mol2' % (args.input_pdbbind, name)))\n",
    "\n",
    "                    except:\n",
    "                        error = \"no pocket for {} ({} set)\".format(\n",
    "                            name, dataset_name)\n",
    "                        warnings.warn(error)\n",
    "                        failure_dict[\"name\"].append(name), failure_dict[\"partition\"].append(\n",
    "                            \"crystal\"), failure_dict[\"set\"].append(dataset_name), failure_dict[\"error\"].append(error)\n",
    "                        continue\n",
    "\n",
    "                    # extract the van der waals radii for the ligand/pocket\n",
    "                    crystal_ligand_vdw = parse_mol_vdw(\n",
    "                        mol=crystal_ligand, element_dict=element_dict)\n",
    "\n",
    "                    # in some, albeit strange, cases the pocket consists purely of hydrogen, skip over these if that is the case\n",
    "                    if len(crystal_ligand_vdw) < 1:\n",
    "                        error = \"{} ligand consists purely of hydrogen, no heavy atoms to featurize\".format(\n",
    "                            name)\n",
    "                        warnings.warn(error)\n",
    "                        failure_dict[\"name\"].append(name), failure_dict[\"partition\"].append(\n",
    "                            \"crystal\"), failure_dict[\"set\"].append(dataset_name), failure_dict[\"error\"].append(error)\n",
    "                        continue\n",
    "\n",
    "                    crystal_pocket_vdw = parse_mol_vdw(\n",
    "                        mol=crystal_pocket, element_dict=element_dict)\n",
    "                    # in some, albeit strange, cases the pocket consists purely of hydrogen, skip over these if that is the case\n",
    "                    if len(crystal_pocket_vdw) < 1:\n",
    "                        error = \"{} pocket consists purely of hydrogen, no heavy atoms to featurize\".format(\n",
    "                            name)\n",
    "                        warnings.warn(error)\n",
    "                        failure_dict[\"name\"].append(name), failure_dict[\"partition\"].append(\n",
    "                            \"crystal\"), failure_dict[\"set\"].append(dataset_name), failure_dict[\"error\"].append(error)\n",
    "                        continue\n",
    "\n",
    "                    crystal_ligand_pocket_vdw = np.concatenate(\n",
    "                        [crystal_ligand_vdw.reshape(-1), crystal_pocket_vdw.reshape(-1)], axis=0)\n",
    "                    try:\n",
    "                        crystal_data = featurize_pybel_complex(\n",
    "                            ligand_mol=crystal_ligand, pocket_mol=crystal_pocket, name=name, dataset_name=dataset_name)\n",
    "                    except RuntimeError as error:\n",
    "                        failure_dict[\"name\"].append(name), failure_dict[\"partition\"].append(\n",
    "                            \"crystal\"), failure_dict[\"set\"].append(dataset_name), failure_dict[\"error\"].append(error)\n",
    "                        continue\n",
    "\n",
    "                    # enforce a constraint that the number of atoms for which we have features is equal to number for which we have VDW radii\n",
    "                    assert crystal_ligand_pocket_vdw.shape[0] == crystal_data.shape[0]\n",
    "\n",
    "                    # END QUALITY CONTROL: made it past the try/except blocks....now featurize the data and store into the .hdf file\n",
    "                    crystal_grp = processed_grp.create_group(\"pdbbind\")\n",
    "                    crystal_grp.attrs[\"van_der_waals\"] = crystal_ligand_pocket_vdw\n",
    "                    crystal_dataset = crystal_grp.create_dataset(\"data\", data=crystal_data,\n",
    "                                                                 shape=crystal_data.shape, dtype='float32', compression='lzf')\n",
    "\n",
    "    failure_df = pd.DataFrame(failure_dict)\n",
    "    failure_df.to_csv(\n",
    "        \"{}/failure_summary.csv\".format(args.output), index=False)\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b80f3a-edd2-4c68-bd88-667ba9c35204",
   "metadata": {},
   "source": [
    "## 6. Generate 3D representation for 3D CNN training\n",
    "The 3D atomic representation is described as follows. The input volume dimension is N × N × N × C, where N is the voxel grid size in each axis and C is\n",
    "the number of atomic features described in the previous section (19 in our experiment). We chose N=48. The volume size in each dimension is approximately 48 Å, where each voxel size is 1 Å, which is sufficient to cover the entire pocket region while minimizing the collisions between atoms. Each atom is assigned to at least one voxel or more, depending on its Van der Waals radius or the user-defined size. In the case of the collisions between atoms, we apply element-wise addition to the atom features. Once all atoms are voxelized, Gaussian blur with σ = 1 is applied to populate the atom features into neighboring voxels and to avoid too sparse of a representationin the 3D voxel grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42b76ad-a37b-4471-b07e-c9ae665c2d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import argparse\n",
    "import csv\n",
    "import h5py\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.ndimage\n",
    "\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    \"--main-dir\", default=\"/workspace/data/processed\", help=\"main dataset directory\") # Path of the hdf filed\n",
    "parser.add_argument(\"--use-external\", default=True,\n",
    "                    help=\"whether external test file is used or not\") # Always set to True\n",
    "parser.add_argument(\"--input-file\", default=\"pdbbind2016_core_test.hdf\", help=\"input test HDF filename\") # Change for general/refined and train/val\n",
    "parser.add_argument(\"--output-file\", default=\"core_test_3dnn.hdf\",\n",
    "                    help=\"output test HDF filename\") # Name of the output file that it will create\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "# do not change unless the hdf structure is changed\n",
    "g_csv_header = ['ligand_id', 'file_prefix', 'label', 'train_test_split', 'atom_count',\n",
    "                'xsize', 'ysize', 'zsize', 'p_atom_count1', 'p_atom_count2', 'p_xsize', 'p_ysize', 'p_zsize']\n",
    "\n",
    "g_feat_tool_list = ['pybel', 'rdkit']\n",
    "g_feat_tool_ind = 0\n",
    "\n",
    "g_feat_type_list = ['raw', 'processed']\n",
    "g_feat_type_ind = 1\n",
    "\n",
    "g_feat_pdbbind_type_list = ['crystal', 'docking']  # for display\n",
    "g_feat_pdbbind_type_list2 = ['pdbbind', 'docking']  # for reading hdf\n",
    "g_feat_pdbbind_type_ind = 0\n",
    "\n",
    "g_feat_data_str = 'data'\n",
    "\n",
    "\n",
    "g_main_dir = \"/workspace/data/processed\"\n",
    "\n",
    "g_target_dataset = \"pdbbind2016\"\n",
    "g_target_trainval_type = \"refined\"\n",
    "g_external_test = True  # default is False\n",
    "if g_external_test:\n",
    "    g_feat_suffix = \"%s_%s_%s\" % (\n",
    "        g_feat_tool_list[g_feat_tool_ind], g_feat_type_list[g_feat_type_ind], g_feat_pdbbind_type_list[g_feat_pdbbind_type_ind])\n",
    "else:\n",
    "    g_feat_suffix = \"%s_%s_%s_%s_%s\" % (g_target_dataset, g_target_trainval_type,\n",
    "                                        g_feat_tool_list[g_feat_tool_ind], g_feat_type_list[g_feat_type_ind], g_feat_pdbbind_type_list[g_feat_pdbbind_type_ind])\n",
    "\n",
    "g_3D_relative_size = False\n",
    "g_3D_size_angstrom = 48  # valid only when g_3D_relative_size = False\n",
    "g_3D_size_dim = 48  # 48\n",
    "g_3D_atom_radius = 1\n",
    "g_3D_atom_radii = False\n",
    "g_3D_sigma = 1\n",
    "if g_feat_tool_ind == 0:\n",
    "    g_3D_dim = [g_3D_size_dim, g_3D_size_dim, g_3D_size_dim, 19]\n",
    "else:\n",
    "    g_3D_dim = [g_3D_size_dim, g_3D_size_dim, g_3D_size_dim, 75]\n",
    "\n",
    "size_angstrom = g_3D_size_angstrom\n",
    "if g_3D_relative_size:\n",
    "    size_angstrom = 0\n",
    "\n",
    "if g_3D_atom_radii:\n",
    "    g_3D_suffix = \"%d_%d_radii_sigma%d_rot0\" % (\n",
    "        size_angstrom, g_3D_size_dim, g_3D_sigma)\n",
    "else:\n",
    "    g_3D_suffix = \"%d_%d_radius%d_sigma%d_rot0\" % (\n",
    "        size_angstrom, g_3D_size_dim, g_3D_atom_radius, g_3D_sigma)\n",
    "\n",
    "if g_external_test:\n",
    "    g_input_test_hd_file = \"core_test.hdf\"\n",
    "else:\n",
    "    g_input_train_hd_file = \"%s_%s_train.hdf\" % (\n",
    "        g_target_dataset, g_target_trainval_type)\n",
    "    g_input_val_hd_file = \"%s_%s_val.hdf\" % (\n",
    "        g_target_dataset, g_target_trainval_type)\n",
    "    g_input_test_hd_file = \"%s_core_test.hdf\" % (g_target_dataset)\n",
    "\n",
    "g_output_hd_compress = True\n",
    "g_output_train_hd_file = \"%s_%s_train.hdf\" % (g_feat_suffix, g_3D_suffix)\n",
    "g_output_val_hd_file = \"%s_%s_val.hdf\" % (g_feat_suffix, g_3D_suffix)\n",
    "g_output_test_hd_file = \"%s_%s_test.hdf\" % (g_feat_suffix, g_3D_suffix)\n",
    "g_output_csv = \"%s_%s_info.csv\" % (g_feat_suffix, g_3D_suffix)\n",
    "\n",
    "\n",
    "# for argument setting\n",
    "g_main_dir = args.main_dir\n",
    "if args.use_external:\n",
    "    g_input_test_hd_file = args.input_file\n",
    "    g_output_test_hd_file = args.output_file\n",
    "    g_output_csv = args.output_file[:-4] + \".csv\"\n",
    "\n",
    "\n",
    "def rotate_3D(input_data):\n",
    "    rotation_angle = np.random.uniform() * 2 * np.pi\n",
    "    cosval = np.cos(rotation_angle)\n",
    "    sinval = np.sin(rotation_angle)\n",
    "    rotation_matrix = np.array(\n",
    "        [[cosval, 0, sinval], [0, 1, 0], [-sinval, 0, cosval]])\n",
    "    #rotated_data = np.zeros(input_data.shape, dtype=np.float32)\n",
    "    rotated_data = np.dot(input_data, rotation_matrix)\n",
    "    return rotated_data\n",
    "\n",
    "\n",
    "def get_3D_bound(xyz_array):\n",
    "    xmin = min(xyz_array[:, 0])\n",
    "    ymin = min(xyz_array[:, 1])\n",
    "    zmin = min(xyz_array[:, 2])\n",
    "    xmax = max(xyz_array[:, 0])\n",
    "    ymax = max(xyz_array[:, 1])\n",
    "    zmax = max(xyz_array[:, 2])\n",
    "    return xmin, ymin, zmin, xmax, ymax, zmax\n",
    "\n",
    "\n",
    "def get_3D_all(xyz, feat, vol_dim, xmin, ymin, zmin, xmax, ymax, zmax, atom_radius=1, atomtype_ind=-1, sigma=0):\n",
    "\n",
    "    # initialize volume\n",
    "    vol_data = np.zeros(\n",
    "        (vol_dim[0], vol_dim[1], vol_dim[2], vol_dim[3]), dtype=np.float32)\n",
    "    vol_tag = np.zeros((vol_dim[0], vol_dim[1], vol_dim[2]), dtype=np.int32)\n",
    "\n",
    "    # voxel size (assum voxel size is the same in all axis\n",
    "    vox_size = (zmax - zmin) / vol_dim[0]\n",
    "\n",
    "    # assign xyz (only center)\n",
    "    for ind in range(xyz.shape[0]):\n",
    "        x = xyz[ind, 0]\n",
    "        y = xyz[ind, 1]\n",
    "        z = xyz[ind, 2]\n",
    "        if x < xmin or x > xmax or y < ymin or y > ymax or z < zmin or z > zmax:\n",
    "            continue\n",
    "\n",
    "        cx = (x - xmin) / (xmax - xmin) * (vol_dim[2] - 1)\n",
    "        cy = (y - ymin) / (ymax - ymin) * (vol_dim[1] - 1)\n",
    "        cz = (z - zmin) / (zmax - zmin) * (vol_dim[0] - 1)\n",
    "\n",
    "        vol_tag[int(cz), int(cy), int(cx)] += 1\n",
    "        if vol_tag[int(cz), int(cy), int(cx)] == 1:\n",
    "            vol_data[int(cz), int(cy), int(cx), :] = feat[ind, :]\n",
    "\n",
    "    # assign xyz\n",
    "    for ind in range(xyz.shape[0]):\n",
    "        x = xyz[ind, 0]\n",
    "        y = xyz[ind, 1]\n",
    "        z = xyz[ind, 2]\n",
    "        if x < xmin or x > xmax or y < ymin or y > ymax or z < zmin or z > zmax:\n",
    "            continue\n",
    "\n",
    "        # compute van der Waals radius and atomic density, use 1 if not available\n",
    "        if atomtype_ind >= 0:\n",
    "            vdw_radius = g_atom_vdw_ligand[feat[ind, atomtype_ind]]\n",
    "            atom_radius = 1 + vdw_radius * vox_size\n",
    "\n",
    "        # setup atom ranges\n",
    "        cx = (x - xmin) / (xmax - xmin) * (vol_dim[2] - 1)\n",
    "        cy = (y - ymin) / (ymax - ymin) * (vol_dim[1] - 1)\n",
    "        cz = (z - zmin) / (zmax - zmin) * (vol_dim[0] - 1)\n",
    "\n",
    "        vx_from = max(0, int(cx - atom_radius))\n",
    "        vx_to = min(vol_dim[2] - 1, int(cx + atom_radius))\n",
    "        vy_from = max(0, int(cy - atom_radius))\n",
    "        vy_to = min(vol_dim[1] - 1, int(cy + atom_radius))\n",
    "        vz_from = max(0, int(cz - atom_radius))\n",
    "        vz_to = min(vol_dim[0] - 1, int(cz + atom_radius))\n",
    "\n",
    "        # uniform density\n",
    "        for vz in range(vz_from, vz_to + 1):\n",
    "            for vy in range(vy_from, vy_to + 1):\n",
    "                for vx in range(vx_from, vx_to + 1):\n",
    "                    if vol_tag[vz, vy, vx] == 0:\n",
    "                        vol_data[vz, vy, vx, :] = feat[ind, :]\n",
    "\n",
    "    # gaussian filter\n",
    "    if sigma > 0:\n",
    "        for i in range(vol_data.shape[-1]):\n",
    "            vol_data[:, :, :, i] = sp.ndimage.filters.gaussian_filter(\n",
    "                vol_data[:, :, :, i], sigma=sigma, truncate=2)\n",
    "\n",
    "    return vol_data, vol_tag\n",
    "\n",
    "\n",
    "def get_3D_all2(xyz, feat, vol_dim, relative_size=True, size_angstrom=48, atom_radii=None, atom_radius=1, sigma=0):\n",
    "\n",
    "    # get 3d bounding box\n",
    "    xmin, ymin, zmin, xmax, ymax, zmax = get_3D_bound(xyz)\n",
    "\n",
    "    # initialize volume\n",
    "    vol_data = np.zeros(\n",
    "        (vol_dim[0], vol_dim[1], vol_dim[2], vol_dim[3]), dtype=np.float32)\n",
    "\n",
    "    if relative_size:\n",
    "        # voxel size (assum voxel size is the same in all axis\n",
    "        vox_size = float(zmax - zmin) / float(vol_dim[0])\n",
    "    else:\n",
    "        vox_size = float(size_angstrom) / float(vol_dim[0])\n",
    "        xmid = (xmin + xmax) / 2.0\n",
    "        ymid = (ymin + ymax) / 2.0\n",
    "        zmid = (zmin + zmax) / 2.0\n",
    "        xmin = xmid - (size_angstrom / 2)\n",
    "        ymin = ymid - (size_angstrom / 2)\n",
    "        zmin = zmid - (size_angstrom / 2)\n",
    "        xmax = xmid + (size_angstrom / 2)\n",
    "        ymax = ymid + (size_angstrom / 2)\n",
    "        zmax = zmid + (size_angstrom / 2)\n",
    "        vox_size2 = float(size_angstrom) / float(vol_dim[0])\n",
    "        #print(vox_size, vox_size2)\n",
    "\n",
    "    # assign each atom to voxels\n",
    "    for ind in range(xyz.shape[0]):\n",
    "        x = xyz[ind, 0]\n",
    "        y = xyz[ind, 1]\n",
    "        z = xyz[ind, 2]\n",
    "        if x < xmin or x > xmax or y < ymin or y > ymax or z < zmin or z > zmax:\n",
    "            continue\n",
    "\n",
    "        # compute van der Waals radius and atomic density, use 1 if not available\n",
    "        if not atom_radii is None:\n",
    "            vdw_radius = atom_radii[ind]\n",
    "            atom_radius = 1 + vdw_radius * vox_size\n",
    "\n",
    "        cx = (x - xmin) / (xmax - xmin) * (vol_dim[2] - 1)\n",
    "        cy = (y - ymin) / (ymax - ymin) * (vol_dim[1] - 1)\n",
    "        cz = (z - zmin) / (zmax - zmin) * (vol_dim[0] - 1)\n",
    "\n",
    "        vx_from = max(0, int(cx - atom_radius))\n",
    "        vx_to = min(vol_dim[2] - 1, int(cx + atom_radius))\n",
    "        vy_from = max(0, int(cy - atom_radius))\n",
    "        vy_to = min(vol_dim[1] - 1, int(cy + atom_radius))\n",
    "        vz_from = max(0, int(cz - atom_radius))\n",
    "        vz_to = min(vol_dim[0] - 1, int(cz + atom_radius))\n",
    "\n",
    "        for vz in range(vz_from, vz_to + 1):\n",
    "            for vy in range(vy_from, vy_to + 1):\n",
    "                for vx in range(vx_from, vx_to + 1):\n",
    "                    vol_data[vz, vy, vx, :] += feat[ind, :]\n",
    "\n",
    "    # gaussian filter\n",
    "    if sigma > 0:\n",
    "        for i in range(vol_data.shape[-1]):\n",
    "            vol_data[:, :, :, i] = sp.ndimage.filters.gaussian_filter(\n",
    "                vol_data[:, :, :, i], sigma=sigma, truncate=2)\n",
    "\n",
    "    return vol_data\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# start the main script\n",
    "g_prefix = ''\n",
    "\n",
    "if g_external_test:\n",
    "    input_test_hdf = h5py.File(os.path.join(\n",
    "        g_main_dir, g_input_test_hd_file), 'r')\n",
    "    output_test_hdf = h5py.File(os.path.join(\n",
    "        g_main_dir, g_output_test_hd_file), 'w')\n",
    "else:\n",
    "    # open input hd5\n",
    "    input_train_hdf = h5py.File(os.path.join(\n",
    "        g_main_dir, g_input_train_hd_file), 'r')\n",
    "    input_val_hdf = h5py.File(os.path.join(\n",
    "        g_main_dir, g_input_val_hd_file), 'r')\n",
    "    input_test_hdf = h5py.File(os.path.join(\n",
    "        g_main_dir, g_input_test_hd_file), 'r')\n",
    "\n",
    "    # create output hd5\n",
    "    output_train_hdf = h5py.File(os.path.join(\n",
    "        g_main_dir, g_output_train_hd_file), 'w')\n",
    "    output_val_hdf = h5py.File(os.path.join(\n",
    "        g_main_dir, g_output_val_hd_file), 'w')\n",
    "    output_test_hdf = h5py.File(os.path.join(\n",
    "        g_main_dir, g_output_test_hd_file), 'w')\n",
    "\n",
    "# create output csv\n",
    "output_csv_fp = open(os.path.join(g_main_dir, g_output_csv), 'w')\n",
    "output_csv = csv.writer(output_csv_fp, delimiter=',')\n",
    "output_csv.writerow(g_csv_header)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# generate 3D for ligand and complex\n",
    "\n",
    "feat_tool_str = g_feat_tool_list[g_feat_tool_ind]\n",
    "feat_type_str = g_feat_type_list[g_feat_type_ind]\n",
    "feat_pdbbind_str = g_feat_pdbbind_type_list2[g_feat_pdbbind_type_ind]\n",
    "\n",
    "if g_external_test:\n",
    "    input_hdfs = [input_test_hdf]\n",
    "    output_hdfs = [output_test_hdf]\n",
    "    output_prefixes = [g_output_test_hd_file[:-4]]\n",
    "    traintest_splits = [2]\n",
    "else:\n",
    "    input_hdfs = [input_train_hdf, input_val_hdf, input_test_hdf]\n",
    "    output_hdfs = [output_train_hdf, output_val_hdf, output_test_hdf]\n",
    "    output_prefixes = [g_output_train_hd_file[:-4],\n",
    "                       g_output_val_hd_file[:-4], g_output_test_hd_file[:-4]]\n",
    "    traintest_splits = [0, 1, 2]\n",
    "\n",
    "for input_hdf, output_hdf, output_prefix, split in zip(input_hdfs, output_hdfs, output_prefixes, traintest_splits):\n",
    "    for lig_id in input_hdf.keys():\n",
    "        # if len(g_prefix) > 0 and not lig_id.startswith(g_prefix):\n",
    "        # g_prefixcontinue\n",
    "\n",
    "        feat_tool_list = input_hdf[lig_id]\n",
    "        if not feat_tool_str in feat_tool_list:\n",
    "            continue\n",
    "        feat_type_list = feat_tool_list[feat_tool_str]\n",
    "        if not feat_type_str in feat_type_list:\n",
    "            continue\n",
    "\n",
    "        feat_pdbbind_list = feat_type_list[feat_type_str]\n",
    "        if not feat_pdbbind_str in feat_pdbbind_list:\n",
    "            continue\n",
    "\n",
    "        print(\"processing %s\" % lig_id)\n",
    "\n",
    "        if g_feat_pdbbind_type_ind == 1:\n",
    "            feat_data_0 = feat_pdbbind_list[feat_pdbbind_str]\n",
    "            for n in range(1, 11):  # assuming pose1 to pose10\n",
    "                if not str(n) in feat_data_0:\n",
    "                    continue\n",
    "\n",
    "                feat_data = feat_data_0[str(n)]\n",
    "                input_data = feat_data[g_feat_data_str]\n",
    "                input_radii = None\n",
    "                if g_3D_atom_radii:\n",
    "                    input_radii = feat_data.attrs['van_der_waals']\n",
    "                input_affinity = input_hdf[lig_id].attrs['affinity']\n",
    "\n",
    "                input_xyx = input_data[:, 0:3]\n",
    "                input_feat = input_data[:, 3:]\n",
    "\n",
    "                output_3d_data = get_3D_all2(input_xyx, input_feat, g_3D_dim, g_3D_relative_size,\n",
    "                                             g_3D_size_angstrom, input_radii, g_3D_atom_radius, g_3D_sigma)\n",
    "                print(input_data.shape, 'is converted into ',\n",
    "                      output_3d_data.shape)\n",
    "\n",
    "                lig_id_pose = lig_id + '_' + str(n)\n",
    "                if g_output_hd_compress:\n",
    "                    output_hdf.create_dataset(\n",
    "                        lig_id_pose, data=output_3d_data, shape=output_3d_data.shape, dtype='float32', compression='lzf')\n",
    "                else:\n",
    "                    output_hdf.create_dataset(\n",
    "                        lig_id_pose, data=output_3d_data, shape=output_3d_data.shape, dtype='float32')\n",
    "                output_hdf[lig_id_pose].attrs['affinity'] = input_affinity\n",
    "\n",
    "                lig_prefix = '%d/%s/%s' % (split, output_prefix, lig_id_pose)\n",
    "                output_csv.writerow(\n",
    "                    [lig_id_pose, lig_prefix, input_affinity, split, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "        else:\n",
    "            feat_data = feat_pdbbind_list[feat_pdbbind_str]\n",
    "            input_data = feat_data[g_feat_data_str]\n",
    "            input_radii = None\n",
    "            if g_3D_atom_radii:\n",
    "                input_radii = feat_data.attrs['van_der_waals']\n",
    "            input_affinity = input_hdf[lig_id].attrs['affinity']\n",
    "\n",
    "            input_xyx = input_data[:, 0:3]\n",
    "            input_feat = input_data[:, 3:]\n",
    "\n",
    "            output_3d_data = get_3D_all2(input_xyx, input_feat, g_3D_dim, g_3D_relative_size,\n",
    "                                         g_3D_size_angstrom, input_radii, g_3D_atom_radius, g_3D_sigma)\n",
    "            print(input_data.shape, 'is converted into ', output_3d_data.shape)\n",
    "\n",
    "            #dgroup = output_hdf.create_group(lig_id)\n",
    "            if g_output_hd_compress:\n",
    "                output_hdf.create_dataset(\n",
    "                    lig_id, data=output_3d_data, shape=output_3d_data.shape, dtype='float32', compression='lzf')\n",
    "            else:\n",
    "                output_hdf.create_dataset(\n",
    "                    lig_id, data=output_3d_data, shape=output_3d_data.shape, dtype='float32')\n",
    "            output_hdf[lig_id].attrs['affinity'] = input_affinity\n",
    "\n",
    "            lig_prefix = '%d/%s/%s' % (split, output_prefix, lig_id)\n",
    "            output_csv.writerow(\n",
    "                [lig_id, lig_prefix, input_affinity, split, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "\n",
    "output_csv_fp.close()\n",
    "if g_external_test:\n",
    "    output_test_hdf.close()\n",
    "else:\n",
    "    output_train_hdf.close()\n",
    "    output_val_hdf.close()\n",
    "    output_test_hdf.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292da251-b418-41a0-ab33-6af5b09186d2",
   "metadata": {},
   "source": [
    "Done!\n",
    "\n",
    "The pdbbind2016_general_train.hdf and general_train_3dnn.csv will be the ones given to the training algorithm (this can be changed to the refined set). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07e68ae-3454-4399-bb75-8b16319838b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
